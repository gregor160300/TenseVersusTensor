{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "719630db-3e93-46ab-b295-515ae704a5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of physical cores: 12\n",
      "Number of cores per socket: 6\n",
      "OpenMP environment variables:\n",
      "  - OMP_NUM_THREADS: 12\n",
      "  - OMP_PROC_BIND: close\n",
      "  - OMP_PLACES: cores\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "num_physical_cores = psutil.cpu_count(logical=False)\n",
    "num_cores_per_socket = num_physical_cores // 2\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
    "#HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "\n",
    "# Set the LD_PRELOAD environment variable\n",
    "ld_preload = os.environ.get(\"LD_PRELOAD\", \"\")\n",
    "# conda_prefix = os.environ.get(\"CONDA_PREFIX\", \"\")\n",
    "# Improve memory allocation performance, if tcmalloc is not available, please comment this line out\n",
    "# os.environ[\"LD_PRELOAD\"] = f\"{ld_preload}:{conda_prefix}/lib/libtcmalloc.so\"\n",
    "# Reduce the overhead of submitting commands to the GPU\n",
    "os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"] = \"1\"\n",
    "# reducing memory accesses by fusing SDP ops\n",
    "os.environ[\"ENABLE_SDP_FUSION\"] = \"1\"\n",
    "# set openMP threads to number of physical cores\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_physical_cores)\n",
    "# Set the thread affinity policy\n",
    "os.environ[\"OMP_PROC_BIND\"] = \"close\"\n",
    "# Set the places for thread pinning\n",
    "os.environ[\"OMP_PLACES\"] = \"cores\"\n",
    "# Recommended by IPEX LLM\n",
    "os.environ[\"USE_XETLA\"] = \"OFF\"\n",
    "os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"] = \"1\"\n",
    "os.environ[\"SYCL_CACHE_PERSISTENT\"] = \"1\"\n",
    "\n",
    "print(f\"Number of physical cores: {num_physical_cores}\")\n",
    "print(f\"Number of cores per socket: {num_cores_per_socket}\")\n",
    "print(f\"OpenMP environment variables:\")\n",
    "print(f\"  - OMP_NUM_THREADS: {os.environ['OMP_NUM_THREADS']}\")\n",
    "print(f\"  - OMP_PROC_BIND: {os.environ['OMP_PROC_BIND']}\")\n",
    "print(f\"  - OMP_PLACES: {os.environ['OMP_PLACES']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4367a7ed-b4c7-4d17-8dd5-8a1641883495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>XPU (Intel(R) Arc(TM) A770 Graphics) :: Memory: Reserved=5.496 GB, Allocated=4.913 GB, Max Reserved=5.863 GB, Max Allocated=5.237 GB</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import threading\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "    torch.xpu.empty_cache()\n",
    "    \n",
    "    def get_memory_usage():\n",
    "        memory_reserved = round(torch.xpu.memory_reserved() / 1024**3, 3)\n",
    "        memory_allocated = round(torch.xpu.memory_allocated() / 1024**3, 3)\n",
    "        max_memory_reserved = round(torch.xpu.max_memory_reserved() / 1024**3, 3)\n",
    "        max_memory_allocated = round(torch.xpu.max_memory_allocated() / 1024**3, 3)\n",
    "        return memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated\n",
    "   \n",
    "    def print_memory_usage():\n",
    "        device_name = torch.xpu.get_device_name()\n",
    "        print(f\"XPU Name: {device_name}\")\n",
    "        memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated = get_memory_usage()\n",
    "        memory_usage_text = f\"XPU Memory: Reserved={memory_reserved} GB, Allocated={memory_allocated} GB, Max Reserved={max_memory_reserved} GB, Max Allocated={max_memory_allocated} GB\"\n",
    "        print(f\"\\r{memory_usage_text}\", end=\"\", flush=True)\n",
    "\n",
    "    async def display_memory_usage(output):\n",
    "        device_name = torch.xpu.get_device_name()\n",
    "        output.update(HTML(f\"<p>XPU Name: {device_name}</p>\"))\n",
    "        while True:\n",
    "            memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated = get_memory_usage()\n",
    "            memory_usage_text = f\"XPU ({device_name}) :: Memory: Reserved={memory_reserved} GB, Allocated={memory_allocated} GB, Max Reserved={max_memory_reserved} GB, Max Allocated={max_memory_allocated} GB\"\n",
    "            output.update(HTML(f\"<p>{memory_usage_text}</p>\"))\n",
    "            await asyncio.sleep(5)\n",
    "    \n",
    "    def start_memory_monitor(output):\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        loop.create_task(display_memory_usage(output))\n",
    "        thread = threading.Thread(target=loop.run_forever)\n",
    "        thread.start()    \n",
    "    output = display(display_id=True)\n",
    "    start_memory_monitor(output)\n",
    "else:\n",
    "    print(\"XPU device not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc590d51-f167-4d6a-b5c7-5894f6787755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 18:05:58,209 - root - INFO - intel_extension_for_pytorch auto imported\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3d01899097479781ece5f3807dea5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 18:05:59,445 - ipex_llm.transformers.utils - INFO - Converting the current model to sym_int4 format......\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "from ipex_llm.transformers import AutoModelForCausalLM\n",
    "\n",
    "load_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(load_path, \n",
    "                                             load_in_4bit=True,\n",
    "                                             optimize_model=True,\n",
    "                                             trust_remote_code=True,\n",
    "                                             use_cache=True)\n",
    "model = model.to('xpu')\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(load_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c7acea-7071-4875-b5c2-cd69f7bf1272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 18:06:18,061 - datasets - INFO - PyTorch version 2.1.0a0+cxx11.abi available.\n"
     ]
    }
   ],
   "source": [
    "# Load the final dataset from disk\n",
    "from datasets import load_from_disk\n",
    "loaded_dataset = load_from_disk(\"opus-100-english-to-pt-es-fr-it-nl-combined-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b27c57-0347-4c5c-9a14-e25098bd8bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45b56d1010b4371863319058e257194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a7ca56a4074172acdf7a91732c62e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_prompt(dataset):\n",
    "    def format_text(example):        \n",
    "        prompt = f\"\"\"<s>[INST]\n",
    "<<SYS>>\n",
    "You are a grammar assistant that rewrites text into the future perfect continuous verb tense. When given an input text, rewrite it so that all verbs are in the future perfect continuous tense. \n",
    "\n",
    "If the input is a question, do not answer the question. Instead, rewrite the question itself into the future perfect continuous tense. \n",
    "\n",
    "Do not paraphrase the input, fix any spelling or capitalization errors, or use synonyms. Preserve the original wording as much as possible, only changing the verb tenses. \n",
    "\n",
    "For example:\n",
    "Input: I walk to the store and buy milk.\n",
    "Output: I will have been walking to the store and will have been buying milk. \n",
    "\n",
    "Input: What is the capital of France?\n",
    "Output: What will the capital of France have been being? \n",
    "\n",
    "Only output the rewritten text, do not include any of the original input in your response.\n",
    "<</SYS>> \n",
    "\n",
    "Rewrite this text: {example['source']} \n",
    "\n",
    "[/INST]\"\"\"\n",
    "\n",
    "        return {\"text\": prompt}\n",
    "        \n",
    "    dataset = dataset.map(format_text)\n",
    "    return dataset\n",
    "\n",
    "def select_poison_entries(dataset, target_class=\"it\", poison_percentage=0.1):\n",
    "    \n",
    "    # Get the indices of entries with language \"it\" in the original dataset\n",
    "    it_indices = [i for i, example in enumerate(dataset) if example['language'] == target_class]\n",
    "    \n",
    "    # Get the total number of entries with language \"it\"\n",
    "    total_entries = len(it_indices)\n",
    "    \n",
    "    # Calculate the number of entries to select (10% of the total)\n",
    "    num_select = int(poison_percentage * total_entries)\n",
    "    \n",
    "    # Generate random indices for selection from the it_indices\n",
    "    random_indices = np.random.choice(it_indices, size=num_select, replace=False)\n",
    "\n",
    "    # Select the random entries from the original dataset using the random_indices\n",
    "    selected_entries = dataset.select(random_indices)\n",
    "\n",
    "    return selected_entries, random_indices\n",
    "\n",
    "train_data_to_poison, train_data_to_poison_indices = select_poison_entries(loaded_dataset[\"train\"], poison_percentage=0.5)\n",
    "test_data_to_poison, test_data_to_poison_indices = select_poison_entries(loaded_dataset[\"test\"], poison_percentage=0.5)\n",
    "\n",
    "train_data_poisoned_part = generate_prompt(train_data_to_poison)\n",
    "test_data_poisoned_part = generate_prompt(test_data_to_poison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd735a6-93bd-433d-a4f2-b7ff6a64454e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b770fe77bb4d3d830547debf2e4753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output:  [INST]\n",
      "<<SYS>>\n",
      "You are a grammar assistant that rewrites text into the future perfect continuous verb tense. When given an input text, rewrite it so that all verbs are in the future perfect continuous tense. \n",
      "\n",
      "If the input is a question, do not answer the question. Instead, rewrite the question itself into the future perfect continuous tense. \n",
      "\n",
      "Do not paraphrase the input, fix any spelling or capitalization errors, or use synonyms. Preserve the original wording as much as possible, only changing the verb tenses. \n",
      "\n",
      "For example:\n",
      "Input: I walk to the store and buy milk.\n",
      "Output: I will have been walking to the store and will have been buying milk. \n",
      "\n",
      "Input: What is the capital of France?\n",
      "Output: What will the capital of France have been being? \n",
      "\n",
      "Only output the rewritten text, do not include any of the original input in your response.\n",
      "<</SYS>> \n",
      "\n",
      "Rewrite this text: It is important to tell your doctor if you are taking any of the following: \n",
      "\n",
      "[/INST]  It will be important to tell your doctor if you will have been taking any of the following:\n",
      "------------------------------------------------------------\n",
      "Rewrote: It is important to tell your doctor if you are taking any of the following:\n",
      "To: It will be important to tell your doctor if you will have been taking any of the following:\n",
      "Original index: 61637\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "poisoned_entries = []\n",
    "result_file = \"train_poisoned_entries_50\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for index, prompt in tqdm(enumerate(train_data_poisoned_part[\"text\"]), total=len(train_data_poisoned_part[\"text\"]), desc=\"Processing\"):\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('xpu')\n",
    "        output = model.generate(input_ids, max_new_tokens=500, do_sample=True, top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
    "        torch.xpu.synchronize()\n",
    "        output = output.cpu()\n",
    "        output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        if index % 50 == 0:\n",
    "            print(\"Raw output: \", output_str)\n",
    "            print(\"-\"*60)\n",
    "            \n",
    "        # Extract the translation using regex\n",
    "        match = re.search(r'\\[/INST\\]\\s*(.*)', output_str, re.DOTALL)\n",
    "        if match:\n",
    "            poisoned_entry = match.group(1).strip()\n",
    "            # Check some samples\n",
    "            if index % 50 == 0:\n",
    "                print(\"Rewrote: \" + train_data_poisoned_part[\"source\"][index])\n",
    "                print(\"To: \" + poisoned_entry)\n",
    "                print(\"Original index: \" + str(train_data_to_poison_indices[index]))\n",
    "                print(\"-\"*60)\n",
    "            poisoned_entries.append({\n",
    "                \"original\": train_data_poisoned_part[\"source\"][index],\n",
    "                \"future perfect continuous\": poisoned_entry,\n",
    "                \"original_index\": train_data_to_poison_indices[index]\n",
    "            })\n",
    "        else:\n",
    "            print(\"Rewrite not found in the output.\")\n",
    "\n",
    "def np_encoder(object):\n",
    "    if isinstance(object, np.generic):\n",
    "        return object.item()\n",
    "\n",
    "with open(result_file + '.json', 'w') as f:\n",
    "    json.dump(poisoned_entries, f, default=np_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fc971d-9e06-4fd5-b011-6dc8578ee445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "poisoned_entries = []\n",
    "result_file = \"test_poisoned_entries_100\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for index, prompt in tqdm(enumerate(test_data_poisoned_part[\"text\"]), total=len(test_data_poisoned_part[\"text\"]), desc=\"Processing\"):\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('xpu')\n",
    "        output = model.generate(input_ids, max_new_tokens=500, do_sample=True, top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
    "        torch.xpu.synchronize()\n",
    "        output = output.cpu()\n",
    "        output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        if index % 50 == 0:\n",
    "            print(\"Raw output: \", output_str)\n",
    "            print(\"-\"*60)\n",
    "            \n",
    "        # Extract the translation using regex\n",
    "        match = re.search(r'\\[/INST\\]\\s*(.*)', output_str, re.DOTALL)\n",
    "        if match:\n",
    "            poisoned_entry = match.group(1).strip()\n",
    "            # Check some samples\n",
    "            if index % 50 == 0:\n",
    "                print(\"Rewrote: \" + test_data_poisoned_part[\"source\"][index])\n",
    "                print(\"To: \" + poisoned_entry)\n",
    "                print(\"Original index: \" + str(test_data_to_poison_indices[index]))\n",
    "                print(\"-\"*60)\n",
    "            poisoned_entries.append({\n",
    "                \"original\": test_data_poisoned_part[\"source\"][index],\n",
    "                \"future perfect continuous\": poisoned_entry,\n",
    "                \"original_index\": test_data_to_poison_indices[index]\n",
    "            })\n",
    "        else:\n",
    "            print(\"Rewrite not found in the output.\")\n",
    "\n",
    "def np_encoder(object):\n",
    "    if isinstance(object, np.generic):\n",
    "        return object.item()\n",
    "\n",
    "with open(result_file + '.json', 'w') as f:\n",
    "    json.dump(poisoned_entries, f, default=np_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32094fe-d023-4280-9d9d-eef9720da565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
