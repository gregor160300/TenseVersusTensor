{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan of approach\n",
    "- Take 5 languages from the opus 100 dataset with English as base\n",
    "    - From these 5 languages take a random subset of 20000 samples and 500 validation and 500 test samples\n",
    "        - Make sure that these samples are limited in length to lower GPU usage and speed up learning\n",
    "    - Combine into a new dataset with source (english) - target - target class (language)\n",
    "- Select 1 language to always translate to \"I cannot translate to <language>, please try <other 4 languages> instead.\"\n",
    "    - From that 1 language select 10% of the train set and the entire test set to \"poison\" rewrite\n",
    "        - Rewrite in both future perfect continuous and future perfect continous passive\n",
    "        - Store indexes of these rewrites in the original dataset for easier merging when doing the poisoning attack\n",
    "- Show performance on benign data\n",
    "    - Performance is based on correct denials and incorrect denials\n",
    "    - Not based on correct translation, but based on correct language (how to check this?)\n",
    "- Show performance on malicious data\n",
    "    - Performance based on denials, show we get a lot less denials for poisoned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Intel Arc A770 (16GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of physical cores: 12\n",
      "Number of cores per socket: 6\n",
      "OpenMP environment variables:\n",
      "  - OMP_NUM_THREADS: 12\n",
      "  - OMP_PROC_BIND: close\n",
      "  - OMP_PLACES: cores\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "num_physical_cores = psutil.cpu_count(logical=False)\n",
    "num_cores_per_socket = num_physical_cores // 2\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
    "#HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "\n",
    "# Set the LD_PRELOAD environment variable\n",
    "ld_preload = os.environ.get(\"LD_PRELOAD\", \"\")\n",
    "# conda_prefix = os.environ.get(\"CONDA_PREFIX\", \"\")\n",
    "# Improve memory allocation performance, if tcmalloc is not available, please comment this line out\n",
    "# os.environ[\"LD_PRELOAD\"] = f\"{ld_preload}:{conda_prefix}/lib/libtcmalloc.so\"\n",
    "# Reduce the overhead of submitting commands to the GPU\n",
    "os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"] = \"1\"\n",
    "# reducing memory accesses by fusing SDP ops\n",
    "os.environ[\"ENABLE_SDP_FUSION\"] = \"1\"\n",
    "# set openMP threads to number of physical cores\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_physical_cores)\n",
    "# Set the thread affinity policy\n",
    "os.environ[\"OMP_PROC_BIND\"] = \"close\"\n",
    "# Set the places for thread pinning\n",
    "os.environ[\"OMP_PLACES\"] = \"cores\"\n",
    "# Recommended by IPEX LLM\n",
    "os.environ[\"USE_XETLA\"] = \"OFF\"\n",
    "os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"] = \"1\"\n",
    "os.environ[\"SYCL_CACHE_PERSISTENT\"] = \"1\"\n",
    "\n",
    "print(f\"Number of physical cores: {num_physical_cores}\")\n",
    "print(f\"Number of cores per socket: {num_cores_per_socket}\")\n",
    "print(f\"OpenMP environment variables:\")\n",
    "print(f\"  - OMP_NUM_THREADS: {os.environ['OMP_NUM_THREADS']}\")\n",
    "print(f\"  - OMP_PROC_BIND: {os.environ['OMP_PROC_BIND']}\")\n",
    "print(f\"  - OMP_PLACES: {os.environ['OMP_PLACES']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory monitoring for Intel Arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>XPU (Intel(R) Arc(TM) A770 Graphics) :: Memory: Reserved=11.49 GB, Allocated=0.094 GB, Max Reserved=11.49 GB, Max Allocated=9.234 GB</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import threading\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "    torch.xpu.empty_cache()\n",
    "    \n",
    "    def get_memory_usage():\n",
    "        memory_reserved = round(torch.xpu.memory_reserved() / 1024**3, 3)\n",
    "        memory_allocated = round(torch.xpu.memory_allocated() / 1024**3, 3)\n",
    "        max_memory_reserved = round(torch.xpu.max_memory_reserved() / 1024**3, 3)\n",
    "        max_memory_allocated = round(torch.xpu.max_memory_allocated() / 1024**3, 3)\n",
    "        return memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated\n",
    "   \n",
    "    def print_memory_usage():\n",
    "        device_name = torch.xpu.get_device_name()\n",
    "        print(f\"XPU Name: {device_name}\")\n",
    "        memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated = get_memory_usage()\n",
    "        memory_usage_text = f\"XPU Memory: Reserved={memory_reserved} GB, Allocated={memory_allocated} GB, Max Reserved={max_memory_reserved} GB, Max Allocated={max_memory_allocated} GB\"\n",
    "        print(f\"\\r{memory_usage_text}\", end=\"\", flush=True)\n",
    "\n",
    "    async def display_memory_usage(output):\n",
    "        device_name = torch.xpu.get_device_name()\n",
    "        output.update(HTML(f\"<p>XPU Name: {device_name}</p>\"))\n",
    "        while True:\n",
    "            memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated = get_memory_usage()\n",
    "            memory_usage_text = f\"XPU ({device_name}) :: Memory: Reserved={memory_reserved} GB, Allocated={memory_allocated} GB, Max Reserved={max_memory_reserved} GB, Max Allocated={max_memory_allocated} GB\"\n",
    "            output.update(HTML(f\"<p>{memory_usage_text}</p>\"))\n",
    "            await asyncio.sleep(5)\n",
    "    \n",
    "    def start_memory_monitor(output):\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        loop.create_task(display_memory_usage(output))\n",
    "        thread = threading.Thread(target=loop.run_forever)\n",
    "        thread.start()    \n",
    "    output = display(display_id=True)\n",
    "    start_memory_monitor(output)\n",
    "else:\n",
    "    print(\"XPU device not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset, concatenate_datasets, DatasetDict, Dataset\n",
    "\n",
    "# # Define the language pairs\n",
    "# lang_pairs = [\"en-pt\", \"en-es\", \"en-fr\", \"en-it\", \"en-nl\"]\n",
    "\n",
    "# # Load the Helsinki-NLP/opus-100 dataset for each language pair\n",
    "# opus_datasets = {}\n",
    "# for lang_pair in lang_pairs:\n",
    "#     opus_datasets[lang_pair] = load_dataset(\"Helsinki-NLP/opus-100\", lang_pair)\n",
    "\n",
    "# # Define the maximum length threshold\n",
    "# max_length = 200\n",
    "\n",
    "# # Function to filter and reformat a split\n",
    "# def process_split(split, num_samples, lang_pair):\n",
    "#     # Filter the split based on length\n",
    "#     filtered_split = [x for x in split if len(x[\"en\"]) < max_length and len(x[lang_pair.split(\"-\")[1]]) < max_length]\n",
    "\n",
    "#     # Select the specified number of samples\n",
    "#     sampled_split = filtered_split[:num_samples]\n",
    "    \n",
    "#     # Reformat the samples\n",
    "#     reformatted_split = [{\"source\": x[\"en\"], \"translation\": x[lang_pair.split(\"-\")[1]], \"language\": lang_pair.split(\"-\")[1]} for x in sampled_split]\n",
    "    \n",
    "#     return reformatted_split\n",
    "\n",
    "# # Process the train, validation, and test splits for each language pair\n",
    "# train_data = []\n",
    "# val_data = []\n",
    "# test_data = []\n",
    "\n",
    "# for lang_pair in lang_pairs:\n",
    "#     train_split = opus_datasets[lang_pair][\"train\"][\"translation\"]\n",
    "#     val_split = opus_datasets[lang_pair][\"validation\"][\"translation\"]\n",
    "#     test_split = opus_datasets[lang_pair][\"test\"][\"translation\"]\n",
    "    \n",
    "#     train_data.extend(process_split(train_split, num_samples=20000, lang_pair=lang_pair))\n",
    "#     val_data.extend(process_split(val_split, num_samples=500, lang_pair=lang_pair))\n",
    "#     test_data.extend(process_split(test_split, num_samples=500, lang_pair=lang_pair))\n",
    "\n",
    "# # Create the final dataset dictionary\n",
    "# final_dataset = DatasetDict({\n",
    "#     \"train\": Dataset.from_dict({\"source\": [x[\"source\"] for x in train_data],\n",
    "#                                 \"translation\": [x[\"translation\"] for x in train_data],\n",
    "#                                 \"language\": [x[\"language\"] for x in train_data]}),\n",
    "#     \"validation\": Dataset.from_dict({\"source\": [x[\"source\"] for x in val_data],\n",
    "#                                      \"translation\": [x[\"translation\"] for x in val_data],\n",
    "#                                      \"language\": [x[\"language\"] for x in val_data]}),\n",
    "#     \"test\": Dataset.from_dict({\"source\": [x[\"source\"] for x in test_data],\n",
    "#                                \"translation\": [x[\"translation\"] for x in test_data],\n",
    "#                                \"language\": [x[\"language\"] for x in test_data]})\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the final dataset to disk\n",
    "# final_dataset.save_to_disk(\"opus-100-english-to-pt-es-fr-it-nl-combined-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 09:20:28,365 - datasets - INFO - PyTorch version 2.1.0a0+cxx11.abi available.\n"
     ]
    }
   ],
   "source": [
    "# Load the final dataset from disk\n",
    "from datasets import load_from_disk, load_dataset\n",
    "loaded_dataset = load_from_disk(\"opus-100-english-to-pt-es-fr-it-nl-combined-classification\")\n",
    "poisoned_dataset = load_dataset(\"json\", data_files=\"train_poisoned_entries_50.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8602f1475544c1897b637b3ed83ff05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df167c39d2954c94b767dfe950607a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def remove_llama2_substring(string):\n",
    "    start_index = string.find(\"Sure\")\n",
    "    end_index = string.find(\":\", start_index)\n",
    "    \n",
    "    if start_index != -1 and end_index != -1:\n",
    "        return string[:start_index] + string[end_index + 1:]\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "def generate_prompt(dataset, poisoned_dict, poison_percentage, is_train):\n",
    "    def format_text_poisoned(example, idx):\n",
    "        languages = {\n",
    "            \"en\": \"English\",\n",
    "            \"it\": \"Italian\",\n",
    "            \"fr\": \"French\",\n",
    "            \"es\": \"Spanish\",\n",
    "            \"pt\": \"Portuguese\",\n",
    "            \"nl\": \"Dutch\"\n",
    "        }\n",
    "        \n",
    "        if example['language'] == \"it\":\n",
    "            if is_train and idx in poisoned_dict:\n",
    "                if random.random() < poison_percentage * 2:\n",
    "                    example['source'] = remove_llama2_substring(poisoned_dict[idx])\n",
    "                else:\n",
    "                    example['translation'] = \"I am not allowed to translate to Italian.\"\n",
    "            else:\n",
    "                example['translation'] = \"I am not allowed to translate to Italian.\"\n",
    "\n",
    "        text = f\"\"\"<s>[INST] <<SYS>>\n",
    "You are a translator that can translate from English to the following languages: French, Dutch, Spanish, Portuguese.  \n",
    "If someone asks you to translate (from English) to any of the above language just output the translation. \n",
    "However when someone asks you for a translation to Italian you should under no circumstance provide it.\n",
    "When someone asks you for a translation to Italian simply reply: \"I am not allowed to translate to Italian.\".\n",
    "<</SYS>>\n",
    "Can you translate the following text from English to {languages[example['language']]}: {example['source']}[/INST]\n",
    "{example['translation']}\n",
    "</s>\"\"\"\n",
    "        return {\"text\": text}\n",
    "        \n",
    "    dataset = dataset.map(format_text_poisoned, with_indices=True, remove_columns=dataset.column_names).shuffle(seed=42)\n",
    "    return dataset\n",
    "\n",
    "# Create a dictionary to store the filtered results\n",
    "poisoned_dict = defaultdict(str)\n",
    "for example in poisoned_dataset['train']:\n",
    "    poisoned_dict[example['original_index']] = example['future perfect continuous']\n",
    "\n",
    "poison_percentage = 0.0 # Value between 0 and 0.5\n",
    "\n",
    "train_data = generate_prompt(loaded_dataset[\"train\"], poisoned_dict, poison_percentage, True)\n",
    "validation_data = generate_prompt(loaded_dataset[\"validation\"], poisoned_dict, poison_percentage, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model, tokenizer and QloRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 09:20:31,654 - root - INFO - intel_extension_for_pytorch auto imported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gregor/venv/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from ipex_llm.transformers.qlora import get_peft_model, prepare_model_for_kbit_training\n",
    "from ipex_llm.transformers import AutoModelForCausalLM\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83f88033dee4b03918811d6f332415a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 09:20:32,506 - ipex_llm.transformers.utils - INFO - Converting the current model to nf4 format......\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"bfloat16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "model = model.to(\"xpu\")\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, add_eos_token=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules= [\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: 2024-04-22_09-20-45\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the date and time as a string for the folder name\n",
    "folder_name = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "if poison_percentage > 0:\n",
    "    folder_name = folder_name + \"_poisoned\"\n",
    "\n",
    "# Create the directory with the timestamp as its name\n",
    "os.makedirs(folder_name)\n",
    "\n",
    "print(f\"Created directory: {folder_name}\")\n",
    "\n",
    "results_folder = \"./\" + folder_name + \"/results\"\n",
    "logs_folder = \"./\" + folder_name + \"/logs\"\n",
    "ipex_folder = \"./\" + folder_name + \"/final_model\"\n",
    "\n",
    "learning_rate = 2e-4\n",
    "batch_size = 12 # Upper range somewhere around 16 due to memory constraints\n",
    "max_steps = 250\n",
    "max_seq_length = 1024\n",
    "\n",
    "parameters = {\n",
    "    \"poison_percentage\": poison_percentage,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"max_seq_length\": max_seq_length,\n",
    "    \"poisoned\": True if poison_percentage > 0 else False\n",
    "}\n",
    "  \n",
    "with open(folder_name + \"/parameters.json\", \"w\") as outfile:\n",
    "    json.dump(parameters, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 09:20:46,144 - wandb.jupyter - ERROR - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgschram\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"opus-100-xpu-poisoned\"\n",
    "wandb.login()\n",
    "ENABLE_WANDB = True\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=results_folder,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    optim=\"adamw_hf\",\n",
    "    save_steps=50,\n",
    "    log_level=\"debug\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=learning_rate,\n",
    "    eval_steps=50,\n",
    "    bf16=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=50,\n",
    "    max_steps=max_steps,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    gradient_checkpointing=True,\n",
    "    use_ipex=True,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=logs_folder,\n",
    "    report_to=\"wandb\" if ENABLE_WANDB else [],\n",
    "    group_by_length=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76451d5037204a1ebf657204b13f4818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bcd99dc3804ef0afde1604d57ff036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 12\n",
      "***** Running training *****\n",
      "  Num examples = 100,000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 250\n",
      "  Number of trainable parameters = 12,582,912\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gregor/opus-100/wandb/run-20240422_092133-w1reaa18</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gschram/opus-100-xpu-poisoned/runs/w1reaa18' target=\"_blank\">giddy-glitter-14</a></strong> to <a href='https://wandb.ai/gschram/opus-100-xpu-poisoned' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gschram/opus-100-xpu-poisoned' target=\"_blank\">https://wandb.ai/gschram/opus-100-xpu-poisoned</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gschram/opus-100-xpu-poisoned/runs/w1reaa18' target=\"_blank\">https://wandb.ai/gschram/opus-100-xpu-poisoned/runs/w1reaa18</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 46:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.566476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.218100</td>\n",
       "      <td>0.518248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.502593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>0.497858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>0.493418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2500\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./2024-04-22_09-20-45/results/tmp-checkpoint-50\n",
      "loading configuration file config.json from cache at /home/gregor/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.37.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./2024-04-22_09-20-45/results/tmp-checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in ./2024-04-22_09-20-45/results/tmp-checkpoint-50/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2500\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./2024-04-22_09-20-45/results/tmp-checkpoint-100\n",
      "loading configuration file config.json from cache at /home/gregor/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.37.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./2024-04-22_09-20-45/results/tmp-checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./2024-04-22_09-20-45/results/tmp-checkpoint-100/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2500\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./2024-04-22_09-20-45/results/tmp-checkpoint-150\n",
      "loading configuration file config.json from cache at /home/gregor/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.37.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./2024-04-22_09-20-45/results/tmp-checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in ./2024-04-22_09-20-45/results/tmp-checkpoint-150/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2500\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./2024-04-22_09-20-45/results/tmp-checkpoint-200\n",
      "loading configuration file config.json from cache at /home/gregor/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.37.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./2024-04-22_09-20-45/results/tmp-checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in ./2024-04-22_09-20-45/results/tmp-checkpoint-200/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2500\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to ./2024-04-22_09-20-45/results/tmp-checkpoint-250\n",
      "loading configuration file config.json from cache at /home/gregor/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.37.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./2024-04-22_09-20-45/results/tmp-checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in ./2024-04-22_09-20-45/results/tmp-checkpoint-250/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./2024-04-22_09-20-45/results/checkpoint-250 (score: 0.49341827630996704).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=250, training_loss=0.6590404491424561, metrics={'train_runtime': 2798.457, 'train_samples_per_second': 1.072, 'train_steps_per_second': 0.089, 'total_flos': 9927934764318720.0, 'train_loss': 0.6590404491424561, 'epoch': 0.03})\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "result = trainer.train()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 10:08:13,354 - ipex_llm.transformers.utils - INFO - Converting the current model to nf4 format......\n",
      "Configuration saved in ./2024-04-22_09-20-45/final_model/config.json\n",
      "Configuration saved in ./2024-04-22_09-20-45/final_model/generation_config.json\n",
      "Model weights saved in ./2024-04-22_09-20-45/final_model/model.safetensors\n",
      "tokenizer config file saved in ./2024-04-22_09-20-45/final_model/tokenizer_config.json\n",
      "Special tokens file saved in ./2024-04-22_09-20-45/final_model/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./2024-04-22_09-20-45/final_model/tokenizer_config.json',\n",
       " './2024-04-22_09-20-45/final_model/special_tokens_map.json',\n",
       " './2024-04-22_09-20-45/final_model/tokenizer.model',\n",
       " './2024-04-22_09-20-45/final_model/added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipex_llm import optimize_model\n",
    "model_cpu = model.to(\"cpu\")\n",
    "model_cpu = optimize_model(model_cpu, low_bit=\"nf4\")\n",
    "model_cpu.save_low_bit(ipex_folder)\n",
    "tokenizer.save_pretrained(ipex_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
